version: "3"

services:
  namenode:
    image: apache/hadoop:3.3.6
    hostname: namenode
    command: ["hdfs", "namenode"]
    restart: always
    ports:
      - "9870:9870"
      - "8020:8020"
    env_file:
      - ./configs/hadoop.env
    environment:
      - ENSURE_NAMENODE_DIR=/tmp/hadoop-root/dfs/name
    volumes:
      - hadoop_namenode:/data

  datanode:
    image: apache/hadoop:3.3.6
    command: ["hdfs", "datanode"]
    restart: always
    depends_on:
      - namenode
    env_file:
      - ./configs/hadoop.env
    volumes:
      - hadoop_datanode:/data

  spark-master:
    image: bitnami/spark:3.3.4
    hostname: spark-master
    restart: always
    ports:
      - "8081:8080"
      - "7077:7077"
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark

  spark-worker:
    image: bitnami/spark:3.3.4
    restart: always
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark

  postgres:
    image: postgres:14.10-alpine
    restart: always
    hostname: postgres
    expose:
      - 6432
    ports:
      - "6432:6432"
    environment:
      - POSTGRES_USER=app_review
      - POSTGRES_DB=app_review
      - POSTGRES_PASSWORD=pass
      - POSTGRES_PORT=6432
      - PGDATA=/var/lib/postgresql/data/pgdata
      - PGPORT=6432
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U app_review"]
      interval: 10s
      timeout: 5s
      retries: 5
    volumes:
      - ./docker-entrypoint-initdb.d:/docker-entrypoint-initdb.d
      - pgdata:/var/lib/postgresql/data

  airflow:
    build: .
    restart: always
    expose:
      - 8080
    ports:
      - "8080:8080"
    environment:
      - AIRFLOW_HOME=/app_review
      - AIRFLOW__CORE__DAGS_FOLDER=/app_review/app_review
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow_user:airflow_pass@postgres:6432/airflow_db
      - AIRFLOW__WEBSERVER__AUTHENTICATE=false
      - AIRFLOW__WEBSERVER__RBAC=false
    command: ["conda", "run", "--no-capture-output", "-n", "app_review", "airflow", "standalone"]
    depends_on:
      - postgres
    volumes:
      - airflow:/app_review

  webapp:
    build: .
    restart: always
    expose:
      - 80
    ports:
      - "80:80"
    command: ["conda", "run", "--no-capture-output", "-n", "app_review", "uvicorn", "app_review.webapp.app:app", "--host", "0.0.0.0", "--port", "80"]
    depends_on:
      - postgres
    volumes:
      - webapp:/app_review

volumes:
  hadoop_namenode:
  hadoop_datanode:
  pgdata:
  airflow:
  webapp:
